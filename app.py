# -*- coding: utf-8 -*-
"""nlp casestudy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eCqIAYZU8_-h4_c_zWgalvACmwtYZGu2
"""

#extract research paper

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
# import kagglehub # Removed KaggleHub import
import os
# import json # Removed json import as we are reading CSV

# Download necessary NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)

# --- Start: Code to read a local CSV file ---
# Please provide the path to your uploaded CSV file here
csv_file_path = 'arXiv_scientific dataset (1).csv' # Replace with your file path

try:
    # Load the data from the CSV file into a pandas DataFrame
    # Assuming the CSV file has columns named 'title' and 'summary' or 'abstract'
    data = pd.read_csv(csv_file_path)
    df = pd.DataFrame(data)

    # Display the column names to check for 'abstract' or 'summary'
    print(df.columns)

    # Check if 'abstract' column exists, otherwise use 'summary'
    abstract_col = 'abstract' if 'abstract' in df.columns else 'summary'
    if abstract_col not in df.columns:
        raise ValueError("Dataset must contain either an 'abstract' or 'summary' column.")

    # 2. Clean the 'title' and 'abstract' (or summary) columns
    def clean_text(text):
        if isinstance(text, str): # Add a check for string type
            text = text.lower()
            text = re.sub(r'[^\w\s]', '', text)
            return text
        return "" # Return empty string for non-string types

    df['title_cleaned'] = df['title'].apply(clean_text)
    df['summary_cleaned'] = df[abstract_col].apply(clean_text) # Use abstract_col

    # 3. Tokenize the cleaned text
    df['title_tokens'] = df['title_cleaned'].apply(word_tokenize)
    df['summary_tokens'] = df['summary_cleaned'].apply(word_tokenize)

    # 4. Remove stop words
    stop_words = set(stopwords.words('english'))

    def remove_stopwords(tokens):
        return [word for word in tokens if word not in stop_words]

    df['title_tokens_no_stopwords'] = df['title_tokens'].apply(remove_stopwords)
    df['summary_tokens_no_stopwords'] = df['summary_tokens'].apply(remove_stopwords)

    # 5. Apply stemming
    stemmer = PorterStemmer()

    def stem_tokens(tokens):
        return [stemmer.stem(word) for word in tokens]

    df['title_stemmed'] = df['title_tokens_no_stopwords'].apply(stem_tokens)
    df['summary_stemmed'] = df['summary_tokens_no_stopwords'].apply(stem_tokens)

    # 6. Combine the preprocessed tokens
    df['preprocessed_text'] = df.apply(lambda row: row['title_stemmed'] + row['summary_stemmed'], axis=1)

    # 7. Join the list of combined tokens into a single string
    df['preprocessed_text_str'] = df['preprocessed_text'].apply(lambda tokens: ' '.join(tokens))

    print(df.head())

except FileNotFoundError:
    print(f"Error: The file '{csv_file_path}' was not found.")
    print("Please upload your CSV file and ensure the 'csv_file_path' variable points to the correct location.")
except ValueError as ve:
     print(f"Error processing dataset: {ve}")
     print("Please check the column names in your CSV file. It should contain 'title' and either 'abstract' or 'summary'.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

# --- End: Code to read a local CSV file ---

from sklearn.feature_extraction.text import TfidfVectorizer

# Instantiate TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the 'preprocessed_text_str' column
tfidf_matrix = tfidf_vectorizer.fit_transform(df['preprocessed_text_str'])

print(tfidf_matrix)

from sklearn.metrics.pairwise import cosine_similarity

def recommend_papers(user_query, tfidf_vectorizer, tfidf_matrix, df):
    """
    Recommends papers based on cosine similarity with a user query.

    Args:
        user_query (str): The user's topic string.
        tfidf_vectorizer: The fitted TfidfVectorizer.
        tfidf_matrix: The TF-IDF matrix of the papers.
        df (pd.DataFrame): The DataFrame containing the paper information.

    Returns:
        pd.Series: A series of similarity scores between the query and each paper.
    """
    # Preprocess the user query
    cleaned_query = clean_text(user_query)
    tokens_query = word_tokenize(cleaned_query)
    tokens_no_stopwords_query = remove_stopwords(tokens_query)
    stemmed_query = stem_tokens(tokens_no_stopwords_query)
    preprocessed_query_str = ' '.join(stemmed_query)

    # Transform the preprocessed query into a TF-IDF vector
    query_tfidf = tfidf_vectorizer.transform([preprocessed_query_str])

    # Calculate cosine similarity
    similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix)

    # Return the similarity scores as a pandas Series
    return pd.Series(similarity_scores[0])

# Example usage (optional, can be removed later)
# user_topic = "artificial intelligence in healthcare"
# similarity_scores = recommend_papers(user_topic, tfidf_vectorizer, tfidf_matrix, df)
# print(similarity_scores)

def recommend_papers(user_query, tfidf_vectorizer, tfidf_matrix, df, N=5):
    """
    Recommends top N papers based on cosine similarity with a user query.

    Args:
        user_query (str): The user's topic string.
        tfidf_vectorizer: The fitted TfidfVectorizer.
        tfidf_matrix: The TF-IDF matrix of the papers.
        df (pd.DataFrame): The DataFrame containing the paper information.
        N (int): The number of top recommendations to return.

    Returns:
        pd.DataFrame: DataFrame containing the top N recommended papers' information.
    """
    # Preprocess the user query
    cleaned_query = clean_text(user_query)
    tokens_query = word_tokenize(cleaned_query)
    tokens_no_stopwords_query = remove_stopwords(tokens_query)
    stemmed_query = stem_tokens(tokens_no_stopwords_query)
    preprocessed_query_str = ' '.join(stemmed_query)

    # Transform the preprocessed query into a TF-IDF vector
    query_tfidf = tfidf_vectorizer.transform([preprocessed_query_str])

    # Calculate cosine similarity
    similarity_scores = cosine_similarity(query_tfidf, tfidf_matrix)

    # Get the indices of the top N most similar papers
    # Use argsort and slicing
    top_n_indices = similarity_scores[0].argsort()[-N:][::-1]

    # Retrieve the information of the top N papers
    recommended_papers_info = df.iloc[top_n_indices]

    return recommended_papers_info

# Example usage:
user_topic = "artificial intelligence in healthcare"
recommended_papers = recommend_papers(user_topic, tfidf_vectorizer, tfidf_matrix, df, N=2)
print(recommended_papers)

















#sentiment analysis


# ==============================
# 1. Install Dependencies
# ==============================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from xgboost import XGBClassifier

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# âœ… Use tf.keras instead of keras standalone
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

from datasets import load_dataset
import pandas as pd

dataset = load_dataset("imdb")

# Convert to DataFrame for easier handling
train_df = pd.DataFrame(dataset["train"])
test_df = pd.DataFrame(dataset["test"])

# Sample a balanced subset
train_df = train_df.sample(n=10000, random_state=42)  # 10k random
test_df = test_df.sample(n=2000, random_state=42)     # 2k random

train_texts = train_df["text"].tolist()
train_labels = train_df["label"].tolist()
test_texts = test_df["text"].tolist()
test_labels = test_df["label"].tolist()

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(
    train_texts, train_labels,
    test_size=0.2,
    random_state=42,
    stratify=train_labels   # âœ… keep class balance
)

# ==============================
# 3. Logistic Regression (TF-IDF)
# ==============================
tfidf = TfidfVectorizer(max_features=10000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf = tfidf.transform(X_val)

log_reg = LogisticRegression(max_iter=200)
log_reg.fit(X_train_tfidf, y_train)

lr_val_preds = log_reg.predict_proba(X_val_tfidf)[:, 1]

# ==============================
# 4. LSTM Model
# ==============================
tokenizer_lstm = Tokenizer(num_words=10000)
tokenizer_lstm.fit_on_texts(X_train)

X_train_seq = tokenizer_lstm.texts_to_sequences(X_train)
X_val_seq = tokenizer_lstm.texts_to_sequences(X_val)

X_train_pad = pad_sequences(X_train_seq, maxlen=200)
X_val_pad = pad_sequences(X_val_seq, maxlen=200)

lstm_model = Sequential([
    Embedding(input_dim=10000, output_dim=64, input_length=200),
    LSTM(64),
    Dense(1, activation='sigmoid')
])

lstm_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
lstm_model.fit(
    X_train_pad, np.array(y_train),
    epochs=2, batch_size=32, verbose=1, validation_split=0.1
)

lstm_val_preds = lstm_model.predict(X_val_pad).flatten()
'''
# ==============================
# 5. BERT Fine-tuning
# ==============================
tokenizer_bert = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer_bert(
    list(X_train), truncation=True, padding=True, max_length=128, return_tensors="pt"
)
val_encodings = tokenizer_bert(
    list(X_val), truncation=True, padding=True, max_length=128, return_tensors="pt"
)

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = IMDbDataset(train_encodings, y_train)
val_dataset = IMDbDataset(val_encodings, y_val)

bert_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=2
)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,   # increase for better performance
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=50,
    eval_strategy="epoch", # Corrected parameter name
    save_strategy="no"
)

trainer = Trainer(
    model=bert_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()

# Get BERT predictions
bert_val_preds = trainer.predict(val_dataset).predictions
bert_val_probs = torch.softmax(torch.tensor(bert_val_preds), dim=1)[:, 1].numpy() '''



# ==============================
# 6. Stacking Ensemble (XGBoost) - UPDATED FOR 2 MODELS
# ==============================
stacked_features = np.vstack([lr_val_preds, lstm_val_preds]).T  # Only LR + LSTM

meta_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
meta_model.fit(stacked_features, y_val)

final_preds = meta_model.predict(stacked_features)

print("\nClassification Report (Stacked Ensemble - LR + LSTM):")
print(classification_report(y_val, final_preds))



# ==============================
# 7. Inference Function (UPDATED - WITHOUT BERT)
# ==============================
def predict_sentiment(text):
    # --- Logistic Regression ---
    tfidf_features = tfidf.transform([text])
    lr_pred = log_reg.predict_proba(tfidf_features)[:, 1]

    # --- LSTM ---
    seq = tokenizer_lstm.texts_to_sequences([text])
    pad_seq = pad_sequences(seq, maxlen=200)
    lstm_pred = lstm_model.predict(pad_seq).flatten()

    # --- Combine Predictions (without BERT) ---
    stacked = np.vstack([lr_pred, lstm_pred]).T  # Only 2 models now
    final_pred = meta_model.predict(stacked)[0]

    sentiment = "Positive ðŸ˜€" if final_pred == 1 else "Negative ðŸ˜¡"
    return sentiment

# ==============================
# 8. Test Custom Inputs
# ==============================
print(predict_sentiment("The movie was absolutely wonderful, I loved it!"))
print(predict_sentiment("Worst film ever, complete waste of time"))
print(predict_sentiment("It was okay, not great but not terrible"))



#news division

# Import libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
X, y = newsgroups.data, newsgroups.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Define models
models = {
    "MultinomialNB": MultinomialNB(),
    "LogisticRegression": LogisticRegression(max_iter=200),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', probability=True)
}

# Map 20 newsgroups into Big-8 Hierarchies
big8_mapping = {
    # Computers
    'comp.graphics': 'Computers', 'comp.os.ms-windows.misc': 'Computers',
    'comp.sys.ibm.pc.hardware': 'Computers', 'comp.sys.mac.hardware': 'Computers',
    'comp.windows.x': 'Computers',

    # Humanities
    'rec.autos': 'Miscellaneous', 'rec.motorcycles': 'Miscellaneous',

    # Miscellaneous
    'misc.forsale': 'Miscellaneous',

    # News
    'talk.politics.guns': 'Talk', 'talk.politics.mideast': 'Talk',
    'talk.politics.misc': 'Talk', 'talk.religion.misc': 'Talk',

    # Sports & Entertainment
    'rec.sport.baseball': 'Sports and Entertainment', 'rec.sport.hockey': 'Sports and Entertainment',

    # Scientific
    'sci.crypt': 'Scientific', 'sci.electronics': 'Scientific', 'sci.med': 'Scientific', 'sci.space': 'Scientific',

    # Social
    'soc.religion.christian': 'Social',

    # Other / fallback
    'alt.atheism': 'Talk', 'news.misc': 'News', 'news.religion.christian': 'Social'
}

# Helper function to map newsgroup names to Big-8
def map_to_big8(newsgroup_name):
    return big8_mapping.get(newsgroup_name, 'Miscellaneous')

# Train and evaluate models
for name, model in models.items():
    print(f"\n--- {name} ---")
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)

    # Convert predictions to Big-8
    y_pred_big8 = [map_to_big8(newsgroups.target_names[i]) for i in y_pred]
    y_test_big8 = [map_to_big8(newsgroups.target_names[i]) for i in y_test]

    print("Accuracy:", accuracy_score(y_test_big8, y_pred_big8))
    print("Classification Report:\n", classification_report(y_test_big8, y_pred_big8))

# Test with a custom news snippet
sample_news = ["NASA is planning to launch a new space telescope next year."]
sample_tfidf = vectorizer.transform(sample_news)
for name, model in models.items():
    predicted_group = newsgroups.target_names[model.predict(sample_tfidf)[0]]
    predicted_big8 = map_to_big8(predicted_group)
    print(f"{name} Prediction: {predicted_big8}")



#readability/ difficulty level

import re
import pandas as pd
from textstat import flesch_reading_ease

class BalancedClassifier:
    def __init__(self, vocab_file):
        self.vocab_df = pd.read_csv(vocab_file)
        self.vocab_levels = dict(zip(
            self.vocab_df['headword'].str.lower(),
            self.vocab_df['CEFR']
        ))

    def classify(self, text):
        """Simple but effective classification"""
        if not text.strip() or len(text.split()) < 8:
            return "elementary"

        # Get basic features
        words = re.findall(r'\b[a-z]+\b', text.lower())
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]

        if not words or not sentences:
            return "elementary"

        # Calculate key metrics
        avg_sentence_length = len(words) / len(sentences)
        readability = flesch_reading_ease(text)

        # Vocabulary analysis - focus on B1/B2 words
        intermediate_words = 0
        advanced_words = 0
        basic_words = 0

        for word in words:
            if word in self.vocab_levels:
                level = self.vocab_levels[word]
                if level in ['A1', 'A2']:
                    basic_words += 1
                elif level in ['B1', 'B2']:
                    intermediate_words += 1
                else:  # C1, C2
                    advanced_words += 1

        total_known_words = basic_words + intermediate_words + advanced_words
        if total_known_words == 0:
            return "elementary"

        intermediate_ratio = intermediate_words / len(words)
        advanced_ratio = advanced_words / len(words)
        basic_ratio = basic_words / len(words)

        # DECISION TREE - Much simpler logic
        # Rule 1: If lots of advanced words â†’ Advanced
        if advanced_ratio > 0.15:  # More than 15% advanced words
            return "advanced"

        # Rule 2: If very simple vocabulary and structure â†’ Elementary
        if (basic_ratio > 0.8 and          # 80%+ basic words
            avg_sentence_length < 12 and   # Short sentences
            readability > 70):             # Very easy to read
            return "elementary"

        # Rule 3: If good mix of intermediate vocabulary â†’ Intermediate
        if (intermediate_ratio > 0.20 or   # At least 20% intermediate words
            (0.10 <= intermediate_ratio <= 0.30 and  # Or balanced mix
             basic_ratio > 0.4 and
             advanced_ratio < 0.08)):
            return "intermediate"

        # Rule 4: Default based on sentence complexity
        if avg_sentence_length > 18:
            return "advanced"
        elif avg_sentence_length > 12:
            return "intermediate"
        else:
            return "elementary"

# TEST WITH SPECIFIC INTERMEDIATE EXAMPLES
classifier = BalancedClassifier('cefrj-vocabulary-profile-1.5.csv')

# Clear intermediate examples that should work
intermediate_texts = [
    # Business/Work
    "The management team decided to implement new procedures to increase productivity. Employees will receive training sessions next week.",

    # Technology
    "Many people use smartphones for daily tasks like banking and shopping. App developers need to ensure their products are secure and user-friendly.",

    # Education
    "Students should develop good study habits to achieve academic success. Regular review of course material helps with long-term retention.",

    # Health/Lifestyle
    "Regular physical activity and a balanced diet contribute to overall wellbeing. Healthcare professionals recommend preventive checkups.",

    # Travel
    "When planning international travel, it's important to check visa requirements and vaccination recommendations. Travel insurance provides additional protection.",

    # Environment
    "Recycling programs help communities reduce waste and protect natural resources. Many cities now offer separate collection for different materials."
]

# Elementary examples
elementary_texts = [
    "I like my school. My friends are nice. We play games together.",
    "She works in a shop. She helps customers. They buy things.",
    "The sun is hot. We go to the beach. We swim in the water.",
    "My family eats dinner together. We talk about our day.",
    "I have a red bike. I ride it in the park."
]

# Advanced examples
advanced_texts = [
    "The epistemological ramifications of quantum entanglement challenge conventional ontological frameworks.",
    "Contemporary geopolitical dynamics necessitate multilateral diplomatic engagement.",
    "Neuroplasticity research elucidates the cerebral cortex's adaptive capabilities."
]

# First, let me run your existing code to set up the classifier
import re
import pandas as pd
from textstat import flesch_reading_ease

class BalancedClassifier:
    def __init__(self, vocab_file):
        self.vocab_df = pd.read_csv(vocab_file)
        self.vocab_levels = dict(zip(
            self.vocab_df['headword'].str.lower(),
            self.vocab_df['CEFR']
        ))

    def classify(self, text):
        """Simple but effective classification"""
        if not text.strip() or len(text.split()) < 8:
            return "elementary"

        # Get basic features
        words = re.findall(r'\b[a-z]+\b', text.lower())
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]

        if not words or not sentences:
            return "elementary"

        # Calculate key metrics
        avg_sentence_length = len(words) / len(sentences)
        readability = flesch_reading_ease(text)

        # Vocabulary analysis - focus on B1/B2 words
        intermediate_words = 0
        advanced_words = 0
        basic_words = 0

        for word in words:
            if word in self.vocab_levels:
                level = self.vocab_levels[word]
                if level in ['A1', 'A2']:
                    basic_words += 1
                elif level in ['B1', 'B2']:
                    intermediate_words += 1
                else:  # C1, C2
                    advanced_words += 1

        total_known_words = basic_words + intermediate_words + advanced_words
        if total_known_words == 0:
            return "elementary"

        intermediate_ratio = intermediate_words / len(words)
        advanced_ratio = advanced_words / len(words)
        basic_ratio = basic_words / len(words)

        # DECISION TREE - Much simpler logic
        # Rule 1: If lots of advanced words â†’ Advanced
        if advanced_ratio > 0.15:  # More than 15% advanced words
            return "advanced"

        # Rule 2: If very simple vocabulary and structure â†’ Elementary
        if (basic_ratio > 0.8 and          # 80%+ basic words
            avg_sentence_length < 12 and   # Short sentences
            readability > 70):             # Very easy to read
            return "elementary"

        # Rule 3: If good mix of intermediate vocabulary â†’ Intermediate
        if (intermediate_ratio > 0.20 or   # At least 20% intermediate words
            (0.10 <= intermediate_ratio <= 0.30 and  # Or balanced mix
             basic_ratio > 0.4 and
             advanced_ratio < 0.08)):
            return "intermediate"

        # Rule 4: Default based on sentence complexity
        if avg_sentence_length > 18:
            return "advanced"
        elif avg_sentence_length > 12:
            return "intermediate"
        else:
            return "elementary"

# Initialize the classifier
classifier = BalancedClassifier('cefrj-vocabulary-profile-1.5.csv')

# Now let me test with some input text
input_text = """
Recent advancements in quantum information science have precipitated a paradigm
shift in computational cryptography, with researchers demonstrating scalable fault-tolerant
qubits capable of executing Shorâ€™s algorithm on classically intractable integer factorization
problems. These developments exacerbate the vulnerability of widely deployed asymmetric encryption
schemes, such as RSA and ECC, necessitating the accelerated deployment of post-quantum
cryptographic protocols predicated on lattice-based, hash-based, and multivariate polynomial
frameworks. Concurrently, the integration of topological qubits and error-correcting surface
 codes promises unprecedented coherence times, albeit with substantial engineering and material
 science challenges. Experts caution that without immediate interdisciplinary collaboration
  among physicists, mathematicians, and cybersecurity professionals, the impending quantum era
   could render existing digital security infrastructures obsolescent, thereby
imperiling sensitive financial, governmental, and personal data at a global scale."""

# Classify the input text
level = classifier.classify(input_text)

print("INPUT TEXT CLASSIFICATION")
print("=" * 50)
print(f"Text: {input_text}")
print(f"Classification: {level.upper()}")
print("=" * 50)









import gradio as gr

# Assuming the following functions and variables are defined in previous cells:
# - recommend_papers(user_query, tfidf_vectorizer, tfidf_matrix, df, N=5)
# - tfidf_vectorizer, tfidf_matrix, df (from research paper recommender)
# - predict_sentiment(text) (from sentiment analysis)
# - classifier (from text difficulty classifier)
# - classify_news_interface(news_text) (from news classification)

def combined_interface(task, input_text):
    """
    Combined Gradio interface function to handle different tasks.
    """
    if task == "Research Paper Recommendation":
        # Assuming N=5 for recommendation
        recommended_papers = recommend_papers(input_text, tfidf_vectorizer, tfidf_matrix, df, N=5)
        # Select relevant columns for display
        display_columns = ['title', 'authors', 'summary', 'published_date']
        return recommended_papers[display_columns], None, None
    elif task == "Sentiment Analysis":
        sentiment = predict_sentiment(input_text)
        return None, sentiment, None
    elif task == "Text Difficulty Classification":
        level = classifier.classify(input_text).upper()
        return None, None, level
    # Add News Classification task
    elif task == "News Classification":
        category = classify_news_interface(input_text)
        return None, None, None, category # Need to handle multiple outputs or restructure

    return None, None, None

# Create the Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Unified Text Analysis Interface")
    with gr.Row():
        task_selector = gr.Radio(
            ["Research Paper Recommendation", "Sentiment Analysis", "Text Difficulty Classification", "News Classification"],
            label="Select Task"
        )
    with gr.Row():
        text_input = gr.Textbox(label="Enter Input Text")

    # Define output components for each task
    # Research Paper Recommendation output (DataFrame)
    paper_output = gr.DataFrame(label="Recommended Papers")
    # Sentiment Analysis output (Textbox)
    sentiment_output = gr.Textbox(label="Sentiment")
    # Text Difficulty Classification output (Textbox)
    difficulty_output = gr.Textbox(label="Difficulty Level")
    # News Classification output (Textbox)
    news_category_output = gr.Textbox(label="News Category")


    # Connect the inputs and outputs to the combined function
    # Need to handle conditional outputs based on task selection
    # A simple approach is to update all outputs and let Gradio handle visibility,
    # or use gr.State and gr.Request to manage state and conditional updates.
    # For simplicity here, we'll update all and rely on Gradio's default behavior
    # or manual clearing if needed.

    # Let's create separate functions for each task to make output handling clearer
    def run_recommendation(query):
        if query:
             recommended_papers = recommend_papers(query, tfidf_vectorizer, tfidf_matrix, df, N=5)
             display_columns = ['title', 'authors', 'summary', 'published_date']
             return recommended_papers[display_columns]
        return pd.DataFrame() # Return empty DataFrame if no query

    def run_sentiment(text):
        if text:
            return predict_sentiment(text)
        return ""

    def run_difficulty(text):
        if text:
            return classifier.classify(text).upper()
        return ""

    def run_news_classification(text):
        if text:
            return classify_news_interface(text)
        return ""


    # Using gr.State to manage task selection and trigger updates
    task_state = gr.State(None)

    def update_task_state(task):
        return task

    task_selector.change(update_task_state, inputs=task_selector, outputs=task_state)

    def process_input(task, input_text):
        if task == "Research Paper Recommendation":
            return run_recommendation(input_text), "", "", ""
        elif task == "Sentiment Analysis":
            return pd.DataFrame(), run_sentiment(input_text), "", ""
        elif task == "Text Difficulty Classification":
            return pd.DataFrame(), "", run_difficulty(input_text), ""
        elif task == "News Classification":
            return pd.DataFrame(), "", "", run_news_classification(input_text)
        return pd.DataFrame(), "", "", "" # Default empty outputs

    text_input.submit(
        process_input,
        inputs=[task_state, text_input],
        outputs=[paper_output, sentiment_output, difficulty_output, news_category_output]
    )

    # Clear outputs when task changes
    def clear_outputs():
        return pd.DataFrame(), "", "", ""

    task_selector.change(
        clear_outputs,
        inputs=None,
        outputs=[paper_output, sentiment_output, difficulty_output, news_category_output],
        queue=False # Don't queue this event
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)

